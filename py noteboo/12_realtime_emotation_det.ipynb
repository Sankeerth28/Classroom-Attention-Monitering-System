{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cd58342",
   "metadata": {},
   "source": [
    "## Realtime emotation detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f648e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import mediapipe as mp\n",
    "\n",
    "# Load the emotion classification model\n",
    "model = load_model(\"G:/zolo/classroom-attention-monitor/models/emotion_model.h5\")\n",
    "\n",
    "# Emotion classes\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# MediaPipe Face Mesh setup\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=False)\n",
    "drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "IMG_SIZE = 48  # FER-2013 input size\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = face_mesh.process(rgb)\n",
    "\n",
    "    label = \"No Face Detected\"\n",
    "\n",
    "    if result.multi_face_landmarks:\n",
    "        face = result.multi_face_landmarks[0]\n",
    "        lm = face.landmark\n",
    "\n",
    "        # Get bounding box\n",
    "        x_coords = [int(l.x * w) for l in lm]\n",
    "        y_coords = [int(l.y * h) for l in lm]\n",
    "        x_min, x_max = max(min(x_coords) - 10, 0), min(max(x_coords) + 10, w)\n",
    "        y_min, y_max = max(min(y_coords) - 10, 0), min(max(y_coords) + 10, h)\n",
    "\n",
    "        face_img = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "        if face_img.size > 0:\n",
    "            gray = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)\n",
    "            resized = cv2.resize(gray, (IMG_SIZE, IMG_SIZE)) / 255.0\n",
    "            input_img = resized.reshape(1, IMG_SIZE, IMG_SIZE, 1)\n",
    "\n",
    "            pred = model.predict(input_img, verbose=0)\n",
    "            emotion = emotion_labels[np.argmax(pred)]\n",
    "            label = f\"{emotion}\"\n",
    "\n",
    "            # Draw bounding box and label\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 255), 2)\n",
    "            cv2.putText(frame, label, (x_min, y_min - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "    # Show emotion label on frame\n",
    "    cv2.putText(frame, f\"Emotion: {label}\", (20, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Emotion Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d535675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cam-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
